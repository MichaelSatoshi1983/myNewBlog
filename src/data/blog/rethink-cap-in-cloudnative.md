---
title: "重新思考：云原生时代的CAP定理权衡之术"
date: "2025-12-26"
pubDatetime: 2025-12-26
description: "本文探讨了CAP定理在云原生时代的失效与演变，论证PACELC定理更好地反映现代分布式系统的真实权衡——从处理罕见分区事件转向管理正常运行下的延迟与一致性矛盾。"
tags: ["懂点云计算"]
---
title: "重新思考：云原生时代的 CAP 定理权衡之术"

date: "2025-12-26"

pubDatetime: 2025-12-26

description: "本文探讨了 CAP 定理在云原生时代的失效与演变，论证 PACELC 定理更好地反映现代分布式系统的真实权衡——从处理罕见分区事件转向管理正常运行下的延迟与一致性矛盾。"

tags: ["懂点云计算"]

自从 Eric Brewer 在 2000 年提出 CAP 定理之后（也就是一致性 Consistency、可用性 Availability、分区容错性 Partition Tolerance），这个理论一直都是分布式系统设计的基础。很长时间以来工程界都普遍将他简化为三选二和零和博弈，也就是在网络分区发生的时候，系统就必须在数据一致性和服务可用性之间做出二选一的选择。然而在云原生架构普及的今天和现代化的数据中心的网络模型的金华以及心得一致性协议的诞生，这个理论逐渐有些站不住脚了。特别是在 Amazon Aurora 和 Google Spanner 这种 NewSQL 数据库出现之后，系统似乎在实践中达成了一致性和高可用性的并存，这就推出了一个新的问题：CAP 定理在现在的今天还适用 ？

今天我就来好好地重新思考这个问题，其实核心点很简单：在现代云原生环境下，网络分区虽然说实在物理上面无解，但是发生的概率已经很低很低了。**因此在设计系统的时候重心已经从处理罕见的灾难性的故障转移到了正常运行下的性能权衡。**在这里我们引入 Daniel Abadi 提出的 PACELC 定理（如果发生 P，就在 AC 之间权衡，否则 E，在 L（延迟）和 C（一致性）之间权衡）在这个概念下，延迟取代了可用性，成为这场权衡战争里的主角。

# CAP 定理的历史局限性和现代解构

## 三选二困境

这里再介绍一下历史吧。

CAP 定理最初是由加州大学伯克利分校的 Eric Brewer 教授在 PODC 200 会议上提出来的，然后 Srth Gilbert 和 Nancy Lynch 在 2002 年给出了相关的形式化证明。核心逻辑很简单，就是在一个分布式数据存储系统里面，节点之间会通过异步网络通信，如果网络发生分区（大白话就是节点之前的通信中断了）系统要么就停机保证数据一致性（选 CP），要么就继续服务但是会造成数据不一致（选 AP）。

在互联网早期，NoSQL（比如 Dynamo）在很大程度上是基于对 CAP 定理的解读设计出来的。当时的大家普遍认为在构建 WAN 的超大规模的戏的时候网络分区是不能避免的，所以考虑到用户体验，就得牺牲一个，就牺牲了强一致性，最后只好追求最终一致性。这种三选二的局面，也就导致了大家争论了好久的 SQL 还是 NoSQL。

但是这种二元对立的观点在如今看来有点站不住脚，或者说是粗糙。首先就是 Brewer 自己都在《CAP Twelve Years Later》里面提出来 CAP 理论中其实对于可用性的定义很严格也就是要求任何非故障节点都必须对请求做出响应。这种做法实际上排除掉了所有 Quorum 共识算法，因为只要是发生了分区的情况，少数派的分区的节点就不能完成写入操作，从而被归类为不可用。但是在真是的工程实践里面，只要是多数的节点还能正常的提供服务，那么系统就会被认为是可用的。

## 云原生环境下的网络模型

现在我们处于云原生时代。底层的基建已经发生了质的飞跃，AWS、Google Cloud、Azure 等等云厂商已经构建了稳定性很高的全球骨干网（当然不排除他们自己不出问题哈）这些骨干瓦工之间用多路径路由技术和光纤很大程度上降低了网络分区的概率，在单一可用区与多个可用区之间 RTT 很多情况下都小于 10ms，而且具备物理隔离确保了极高的物理链路稳定性。

在这种环境下，我们之前说到的在互联网早期经常发生的网络分区从常态化事件变成了罕见事件。**如果一个系统在设计的时候仅仅围绕着那个发生概率不到百分之 0.1 的分区情况来做决策，却忽略了百分之 99.9 的正常时刻，那么这个系统就是差劲的。**Martin Kleppmann 在对 CAP 的批判中指出：**现代的系统在设计的时候真正面临的挑战都不是完全的网络中断而是部分分区，也就是网络没有完全断开，但是延迟这个因素变高了。**在这种情况下，CAP 定理很显然就站不住脚，因为他没有把延迟作为一个影响因素纳入考量范围。

所以我们需要一个新的理论框架，能够覆盖系统在非分区状态下的权衡逻辑，这就是为什么我们需要 PACELC。

# PACELC：连续权衡

## 定义与象限

PACELC 定理不单单是对 CAP 的扩展，更像是对分布式系统运行状态的完整的描述，他这个名字由两部分组成（也就是从中间分开）：

- **PAC（Partitioned Availability vs Consistency）**：也就是发生 P 的时候，系统就必须在 A 和 C 之间做选择（也就是延续了 CAP 的逻辑）。
- **ELC（Else Latency vs Consistency）**：否则当系统处于没有分区的正常状态的时候，系统就必须在 L 和 C 之间做权衡。

这一方面的拓展十分重要，因为他填补了 CAP 所没有考虑到的空白，**在系统没有产生分区的正常情况下，系统为了达成一致性的要求，就必须等由网络传输和磁盘 IO 产生的物理时间，这个等待的时间直接就表现在用户的请求延迟上，如果应用无法忍受这种延迟那么就必须放款对一致性的要求从而降低延迟的发生**。

主流数据库在 PACELC 框架下面的分类：

| 系统类型             | PACELC 分类 | 解释                                                         |
| -------------------- | ----------- | ------------------------------------------------------------ |
| DynamoDB / Cassandra | PA/EL       | 分区时优先保可用性（PA）；正常时优先保低延迟（EL），采用最终一致性。 |
| Google Spanner       | PC/EC       | 分区时优先保一致性（PC，停止写入）；正常时优先保一致性（EC），愿意付出延迟代价。 |
| MongoDB              | PA/EL       | 默认异步复制，追求读写速度。但在配置 ReadConcern: Majority 后表现为 PC / EC 特征。 |
| VoltDB / CockroachDB | PC/EC       | 强一致性系统，通过优化共识算法来压缩 E 阶段的延迟成本。      |

## 延迟：这次我是主角！

在 PACELC 的框架下，延迟成了云原生设计架构中的核心约束。对于一个需要在全球范围内部署的微服务应用，用户的请求可能会跨越大洋彼岸才到你这，光速的物理限制就决定了跨越大西洋的 RTT 注定不会太低。如果选择强一致性，那么每次写入就得忍受动辄 50m 的通信成本。

Edward Lee 提出的 CAL 定理进一步把这种关系进行了量化处理。CAL 指出**实际上可用性就是延迟的函数。**对于实时性的系统来说，如果一个延迟的响应超过了业务允许的范围，那么这个系统对于用户来说就是不可用的，即便他最后成功的执行然后返回来了正确的结果。

那么从这个角度来看，现代分布式数据库的竞争点已经从能不能在出现分区的时候活下来转移到了能不能在保证一致性的前提下把延迟压缩到用户察觉不到的程度。换句话说就是大家正在试图通过技术手段让 PC/EC 的系统中的 L 无限趋近于 PA/EL 系统里面的 L，实现一个假 CA 的状态。

# 可恶的物理学：一致性的延迟税

## 光速与序列化的瓶颈

强一致性要求系统的行为表现得就像是只有一个副本那样子，操作是有顺序的而且是原子化的。**为了实现这一点，所有的副本必须在对客户端确认写入操作之前进行通信。**根据狭义相对论的说法，信息传递的速度天花板是光速，在光纤网络中，因为物理层面的折射率的影响，光速大约是真空中原始速度的 2/3，这意味着任何 EC 系统的写入延迟都有一个硬性的下限就是：

$$L_{write} \ge \frac{Distance}{Speed\_of\_Light} \times RTT\_Factor + Processing\_Time$$

我们把目光换到云原生的环境下就是：

- **Intra-AZ**：距离小于 2km，光纤延迟小于 0.1ms。再加上交换机的转发处理，我们 RTT 且当 1ms 看。
- **Inter-AZ**：距离小于 100km，光纤延迟小于 1ms，RTT 就当 2-3ms 看。
- **Inter-Region**：距离大于 3000km，RTT 一定是大于 30ms 的。

对于部署在多可用区的系统中，为了保证谁的不丢失（也就是 RPO=0）每一次 Commit 就必须等待至少一轮的跨 AZ 的 RTT，这就是一致性的延迟税。

**相比之下，EL 系统可以在本地写入之后立刻就返回。**延迟的瓶颈在于本地磁盘的 IO 而不是光速的限制

## 微服务放大与长尾延迟

在微服务架构下，一个用户的请求往往需要调用数十个后端的服务如果说我们的底层数据库选择了 EC，那么每个依赖这个数据库的微服务都已承担额外的延迟，根据 Google 的相关研究表示长尾延迟（比如 P99）会随着 Fan-out 系数的增大而变得指数级增大。

如果一个强一致性的数据库在百分之 99 的时间里面都相应的很快，但是在百分之 1 的时间都在因为等待共识操作而变慢，**那么上层的微服务就一定会频繁超时。**

但是随着业务的复杂化，最后总一致性带来的开发难度和数据修复的成本会越来越大直到难以承受，这迫使我们去寻找新的方案，也就是在 PCEC 框架里面消除关于延迟的影响。

# 革命性时刻：L 被压缩

**如果物理学定律无法打破，那么工程学的目标就是逼近这个物理学极限。**

近年来随着硬件技术的飞跃发展，特别是针对网络硬件的优化，PACELC 的权衡参数正在被一点点改写。

## RDMA：绕过内核的共识加速

在传统的 TCP/IP 网络协议栈中，存在着严重的延迟开销。数据包从网卡到应用程序，需要经过多次内核上下文切换，内存拷贝和 CPU 的中断处理。这就导致即便物理链路很快，但是主机的处理延迟也会有几十微秒的差距。

**远程直接内存访问也就是 RDMA** 技术允许网卡直接读写远程服务器的内存，完全绕过远程 CPU 和操作系统内核。

- **零拷贝与旁路内核**：RDMA 实现了真正的零拷贝，将端到端的延迟降低到了 1-2 微秒的级别。
- **对共识协议的影响**：传统的 Raft 和 Paxos 协议依赖于 RPC 通信，在 APUS 和 DARE 的研究中，研究人员展示了利用 RDMA 的单边原语来优化共识流程，leader 可以直接把日志条目写入到 Follower 的内存环形缓冲区里面，而不需要 Follower 的 CPU 参与进行接收处理。
- **TiDB 和 Azure 的实践**：微软的 Azure 在其云存储后端广泛部署了基于 InfiniBand 和 RoCEv2 的 RDMA 网络，这让跨界复制的延迟几乎等于是本地内存的访问速度。而 TiDB 等新型数据库也开始探索利用 RDMA 来优化 Raft 层的日志复制性能，从而在 PC/EC 的模式下实现极高的吞吐量和极低的延迟。

## P4：网络即共识

除了设备端的优化，网络设备本身也在进化。比如 P4（Programming Protocol-independent Packet Processors）语言允许我们对交换机的数据进行平面编程，这意味着交换机不再仅仅是转发，**而是可以处理应用层的逻辑。**

这一领域有两个突破性的代表：P4xos 和 NetPaxos。通过交换机上实现 Paxos 协议的序列化逻辑，交换机可以直接为通过的数据包分配全局唯一的序列号，甚至直接在交换机层面解决冲突。

- **消灭 RTT：**在传统架构中，leader 需要接受请求排序然后再广播给 Follower。在 P4 架构中，请求在穿过交换机的一瞬间就完成了排序，这实际上将共识协议的一般逻辑都卸载到了网络的路劲上，极大程度的缉拿少了消息的往返次数。
- **确定的延迟：**硬件处理的延迟是确定性的（可以理解为极小的抖动），这对于消除分布式系统中的长尾延迟很重要。

## 原子钟和 TrueTime：用时间换通信

Google Spanner 的出现是分布式系统的历史上重要的一环，它之所以能呈现出 CA 系统的特性（也就是全球分布而且还强一致），核心就在于 TrueTime API。传统的分布式系统因为无法信任物理时钟（因为会产生时钟飘逸），必须去依赖于逻辑时钟（比如说 Lamport Lock 或者说 Vector Lock 之类的）和大量的通信来确定事件产生的先后顺序。Spanner 利用部署在数据中心的 GPS 接收器和原子钟把时间误差限制在了一个很小的范围内。

- **Commit Wait 机制：**Spaner 的写入事务在提交的时候只需要等待极短的实践，只要等待了这个时间窗口，那么根据 TrueTime 的保证，就能确保这个事务的时间戳在物理时间上绝对晚于在此之前提交的任何事物。
- **PACELC：**Spanner 实际上就是在 EC 分支上做到了机制，**虽然他依然有 L，但是它的 L 是一个极短的由高精度硬件确保的时间窗口**而不是一个不确定的网络通信。这种设计把一致性的成本从网络通信的不确定性转化为了等待时间的确定性。

# 现代数据库的权衡之术

## NewSQL 坚守者

Spanner 和 CockroachDB 是 PC 和 EC 阵营的坚守者，他们通过 Paxos 或者是 Raft 协议保证数据在地理分布上的强一致性。

- **Geo-Partition**：为了缓解跨区域的高延迟，CockroachDB 引入了地理分区的概念，数据被放在特定的区域，使得大部分的读写操作的 Raft Group 都在 Intra-Region。这一大部分请求只需要承受极短的 L 而不是高延迟的 L。
- **Global Tables：**对于需要全球访问的数据，**CockroachDB 允许非阻塞读取，但是代价是极高的写入延迟。**简单来说也就是写入操作必须复制到了全球的所有节点才能返回，这是一种极端的 PACELC 权衡之术：**为了全球读的 EL，牺牲了写的低延迟（增加 L 换取 C）**

## 重新定义 Quorum

Amazon 代表了云原生数据的另一种进化路径，传统的 Quorum 是（比如 N=3，W=2，R=2）

$$W+R > N$$

**但是 Aurora 认为 N=3 在云环境中是不够安全的**（比如可能遭受到 AZ 级别的故障+单点故障），因此采用了 N=6，也就是跨越三个 AZ，每个 AZ 存两份。

- **4/6 法则**：Aurora 的写入只要是收到 6 个副本中的 4 个就可以确认标记为成功，这个看似是增加了延迟（因为需要更多的节点去响应），**但是因为他只需要最快的四个，实际上自动屏蔽了慢节点造成的影响。**
- **日志即数据库：**Aurora 并不传输数据页，**它只传输 Redo Log**。这极大的降低了网络带宽的占用，从而变相的减少了复制的延迟。**Aurora 的设计哲学就是针对云基建深度定制，让 EC 的成本可以低到视为 SQL 的标准性能。**

## 选择权给你

如果说我们上面提到的 Spanner 是在尝试去隐藏权衡，那么 AzureCosmos DB 就是把选择权交给用户，因为它提供了五中一致性级别让你去选择：

- **Strong**：对一个 PC/EC，**读写延迟最高，跨区 RTT 是不可避免的缺陷。**
- **Bounded Staleness**：对应 PC/EL，**允许读取旧数据，但是旧的程度有限制（时间限制或者是版本数量限制）**这个可以说是 CAL 的最佳实践，定义允许不一致的窗口来换取更低的延迟
- **Session**：最常用模式，**保证在同一个 Session 内读写一致。**最务实的选择（LC 之间）
- **Consistent Prefix**：**只保证读取顺序****真确****，但是不保证数据最新。**
- **Eventual：**对应 PA/EL，**延迟最低，吞吐量最高。**

这种设计承认了世界上没有万能药，**针对不同的业务场景去配置不同的策略才是最佳实践。**

## TiDB 与 Aurora DSQL：存算分离和大弹簧！

TiDB 采用了计算与存储分离的架构，底层 TiKV 使用 Raft 协议。为了优化 L，TiDB 实现了 Follower Read，也就是说在保证线性一致性的前提下，follower 节点会向 leader 询问最新的 Commit Index（也就是 ReaderIndex）来直接服务于读取的请求，这就省去了日志复制造成的开销，从而只保留一次轻量级而且开销很小的网络交互，进而我们可以发现显著的降低了读取的延迟。

Amazon 最新发布的 Aurora DSQL 则更深入一层，直接上了 Serverless 和 OCC，在没有 E 的常态化下，**他可以实现极低的延迟，但是在发生冲突的时候，他会通过重试操作来试图解决。**这个设计理念实际上把我们上面的 PACELC 里面的 L 转换为了高并发场景下的吞吐量问题。

# 部分分区与 Jepsen 问题

## 部分分区的灾难故障

**传统的 CAP 假设我们的分区是完全隔离的。**但现在环境不同了，k8s 和 services mesh 的出现让部分分区的场景更加常见：比如 A 可以连接 B，B 可以连接 C 但是 A 不能连接 C。这就会造成如下两种情况：

- **对 Raft 的破坏**：在这种情况的拓扑下，A 可能会因为连接不上 leader c 从而不断地发起选举发起选举，导致 Term 无限增加，迫使节点 C 退位，让整个集群都处在 leader 不断切换的动荡状态下，虽然所有的节点都还活着，但是系统实际上是处于无法写入的不可用的状态。
- **PACELC 机制失效了：**这个时候系统就处于一种诡异的状态：**此时系统既不是 P，也不是 E**，这就属于我们上面提到的灰度故障机制，导致 L 趋于无穷大。为了解决这个问题，现代的 Raft 协议引入了 PreVote 机制，要求 Candidate 在发起真正的选举之前先确认自己能不能取得多数派的连接，从而来避免破坏集群稳定性的情况发生。

## Jepsen Test：妖孽哪里逃

Kyle Kingsbury 的 Jepsen Test 测试框架可以被誉为是分布式领域的照妖镜了，是骡子是马拉出来溜溜就知道了。这个测试很简单，就是通过注入各种复杂的网络故障来验证系统是不是真的满足其承诺的一致性模型。

- **MongoDB 的教训：**在早期版本中，MongoDB 即便是配置了 WriteConcern Majority 也会出现在网络分区导致旧的 Primary 可用性没有被降级的情况下，**仍然可能去提供旧数据的情况。**这违反了强一致性的要求，简单来说就是在本来是 PC 的场景下错误的表现了 PA 的行为。
- **ETCD 扛下了所有：**相比上面的情况，ETCD 在 Jepsen Test 测试框架下表现出了严格的线性一致性，也就是说即便在恶劣的网络环境下，Etcd 宁愿报错（牺牲 A）也不会去返回脏数据。这也是为什么 k8s 采用 etcd 来存储元数据的原因：**对于基础设施编排阶段而言，一致性或者说是状态的正确性高于一切。**

通过上面的测试告诉我们 PACELC 的衡量不仅仅是架构的选择，更是超时逻辑和重试逻辑的具体实现。

# 延迟就是一切

## 延迟是可用性的量化体现

上面扯了这么多，我们得出来一个关键的结论：**在云原生时代，可用性不再是一个二元状态，换句话说就是不再是简单的 Up 和 Down，而是延迟的概率分布函数：**

- **概念推理：**假设我们手中有一个设置了 timeout 为 500ms 的微服务，如果说他的底层的数据库因为一致性要求导致 P99 延迟达到了 600ms，那么对于百分之 1 的用户来说，这个系统都是不可用的。
- **结论：**CAP 中的 A 和 PACELC 中的 L 在高并发系统的场景下都是等价的。**当我们在追求极致的可用性的时候，本质上是在追求极致的****低延迟稳定性****。**Google Spanner 证明了只要把 EC 下的 L 压缩的足够小而且稳定（方差低）那么 CP 系统就可以在体验上和 AP 系统相同。

## 这么多税呢之速度税

在选择 EL 的时候我们虽然避开了物理意义上的延迟税，**但是却隐形征收了开发税：**

- 开发者在使用最终一致性数据库的时候，**必须去处理幂等性、乱序到达、ReadYourWrites 失效等等情况**，引入这些因素必然会导致业务代码复杂度上升。
- 越来越多的企业开始愿意支付 Spanner 带来的硬件成本和微小的 Infrastructure Tax，来换取简单的编程模型（比如 ACID）

## 结局：自适应一致性

**我觉得未来的数据库一定不会是静态的 CP 或者说 AP 系统****，而****是和 AI 结合起来，数据库可能会实现自适应 PACELC**：

- 检测到网络波动的时候（Pre Partition）就自动降级为最终一致性保活。
- 当检测到重点事件的时候（比如说大额转账）这个时候就升级为强一致性。

TiDB 的 Placement Driver 和 CockroachDB 的 leaseholder 机制已经有点这种智能调度的样子了。

# 总结

在云原生时代我没去重新审视 CAP 定理的时候，在理论上看，三选二依然成立。**但是在工程角度上来说已经过时了。**

PACELC 定理提供了一个更符合现代网络现实的框架，他指出了延迟才是现在这种情况下才是常态化运行的系统下的一致性代价。

**我们并没有打破物理定律，光速依然在限制跨区域共识的最小 RTT。**但是我们可以通过 RDMA 技术来消除软件栈的开销，我们还可以通过 P4 来卸载共识逻辑，我们也可以通过 TrueTime 技术来压缩时钟的不确定性，以及还可以通过 TiDB 等架构创新去优化机制，我们已经成功的把以前遥不可及的强一致性降低到了商业可接受的范围内。**这就是为什么我很喜欢工程师精神的一点，科学家负责划分理论所规定的界限，工程师负责在围栏中起舞，让遥不可及的理想变得可以伸手去触摸。**

所以最终的结论就是：**云原生时代，我们不再被动的做选择题，我们正在主动地去管理 PACELC 中的权衡。**对于绝大多数的应用而言，有效的一致性（换句话说就是在用户感知的延迟阈值内完成一致性共识）已经成为可能。

**CAP 的诅咒一直存在，但我们学会了与他共舞。**

Make it happen.

Make it tangible.

Make it precise.
